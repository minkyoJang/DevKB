{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1차시"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HTML 문서 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "\n",
      "<html>\n",
      "<head>\n",
      "<meta charset=\"utf-8\"/>\n",
      "<title>Crawl This Page</title>\n",
      "</head>\n",
      "<body>\n",
      "<div class=\"cheshire\">\n",
      "<p>Don't crawl this.</p>\n",
      "</div>\n",
      "<div class=\"elice\">\n",
      "<p>Hello, Python Crawling!</p>\n",
      "</div>\n",
      "</body>\n",
      "</html>\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# index.html을 불러와서 BeautifulSoup 객체를 초기화해 soup에 저장하세요.\n",
    "soup = BeautifulSoup(open('index.html'), \"html.parser\")\n",
    "print(soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### p태그 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Don't crawl this.\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# index.html을 불러와서 BeautifulSoup 객체를 초기화해 soup에 저장하세요.\n",
    "soup = BeautifulSoup(open(\"index.html\"), \"html.parser\")\n",
    "\n",
    "test = soup.find(\"p\").get_text()\n",
    "print(test)\n",
    "\n",
    "# soup를 사용하여 요구되는 정보를 출력해보세요.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, Python Crawling!\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# index.html을 불러와서 BeautifulSoup 객체를 초기화해 soup에 저장하세요.\n",
    "soup = BeautifulSoup(open(\"index.html\"), \"html.parser\")\n",
    "test = soup.find(\"div\", class_ = \"elice\").find(\"p\").get_text()\n",
    "print(test)\n",
    "\n",
    "# soup를 사용하여 요구되는 정보를 출력해보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'get_text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-b1364b35819b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0msoup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"index.html\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"html.parser\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mtest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"div\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mid\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"main\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'get_text'"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# index.html을 불러와서 BeautifulSoup 객체를 초기화해 soup에 저장하세요.\n",
    "soup = BeautifulSoup(open(\"index.html\"), \"html.parser\")\n",
    "\n",
    "test = soup.find(\"div\", id=\"main\").get_text()\n",
    "\n",
    "print(test)\n",
    "# soup를 사용하여 요구되는 정보를 출력해보세요.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2차시 Request"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### requests로 웹 페이지 HTML 문서 불러오기\n",
    "지금까지의 실습에서는 미리 준비한 HTML 문서로 작업하였습니다.\n",
    "\n",
    "하지만, 웹 페이지에 있는 정보를 얻기 위해서는 웹 페이지의 HTML 문서를 얻어야 합니다.\n",
    "\n",
    "requests 라이브러리를 이용하여 https://www.naver.com 페이지의 HTML 문서를 불러와 출력해보세요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 지시사항\n",
    "requests 라이브러리로 주어진 웹 페이지의 HTML 문서를 불러와 출력하세요.\n",
    "\n",
    "주어진 url은 다음과 같습니다.\n",
    "\n",
    "https://www.naver.com\n",
    "\n",
    "주어진 url에 GET 요청을 보내는 방법은\n",
    "\n",
    "url = \"https://www.naver.com\"\n",
    "req = requests.get(url)\n",
    "Copy\n",
    "입니다.\n",
    "\n",
    "req 변수에는 requests 모듈이 해당 URL에 요청을 보낸 것에 대한 응답 결과가 들어있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://www.naver.com\"\n",
    "req = requests.get(url)\n",
    "# url 변수에 담긴 url의 html 문서를 불러와 출력해보세요.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3차시 실전 크롤링"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 네이버 헤드 뉴스 찾기\n",
    "\n",
    "지시사항\n",
    "주어진 코드는 main 함수와 crawling 함수가 있습니다.\n",
    "\n",
    "main 함수에서는 crawling 함수의 결과값을 출력합니다. 수강생 여러분이 main 함수를 직접 수정하실 필요는 없습니다.\n",
    "\n",
    "crawling 함수를 올바르게 구현해주세요.\n",
    "\n",
    "crawling 함수는 네이버 메인 페이지의 헤드 뉴스를 찾고, 그것들의 제목을 담고 있는 리스트를 반환해야 합니다.\n",
    "ㅡ\n",
    "\n",
    "\n",
    "#### Tip!\n",
    "- list.append() : 리스트의 맨 뒤에 요소를 추가하는 파이썬 메소드입니다.\n",
    "- soup.find() : BeautifulSoup 객체에서 특정 태그를 찾기 위해 사용합니다.\n",
    "- soup.find_all() : BeautifulSoup 객체에서 특정 태그 여러 개를 찾아 리스트 자료형으로 얻습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['김총리 \"재난지원금 여력 없다\"…이재명 \"예산은 언제나 부족\"', '한-헝가리 정상회담…文 \"전기차 배터리 등 유망산업 교역확대\"', \"정부-업계, '품귀 요소수' 매점매석 처벌·판매량 제한 협의\", \"비규제지역 주택 사서 양도세 절세 '꼼수' 올해부터 차단\", '\"나 확진자야\" 으름장에 \"확진인데 잠깐 나왔어\" 이탈까지', '부산지하철 1호선, 퇴근시간 34분간 운행 중단', \"검찰, '수사 배제설' 검사도 투입…'대장동팀' 영장심사 종료\", '국민대, 김건희 논문 재검증 계획 교육부에 제출', 'TBS 예산 삭감에 시의회 \"보복성\" vs 서울시 \"충격요법\"', '성남 내곡터널서 화물차 화재…5명 연기흡입 이송']\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def crawling(soup) :\n",
    "    # soup 객체에서 추출해야 하는 정보를 찾고 반환하세요.\n",
    "    result = []\n",
    "    \n",
    "    div = soup.find(\"div\", class_=\"list_issue\")\n",
    "\n",
    "    for a in div.find_all(\"a\"):\n",
    "        result.append(a.get_text())\n",
    "    return result\n",
    "    \n",
    "\n",
    "def main() :\n",
    "    custom_header = {\n",
    "        'referer' : 'https://www.naver.com/',\n",
    "        'user-agent' : 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.103 Safari/537.36'\n",
    "    }\n",
    "    \n",
    "    url = \"http://www.naver.com\"\n",
    "    req = requests.get(url, headers = custom_header)\n",
    "    soup = BeautifulSoup(req.text, \"html.parser\")\n",
    "\n",
    "    # crawling 함수의 결과를 출력합니다.\n",
    "    print(crawling(soup))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\" :\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4차시 크롤링 심화 개념 및 실습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 여러 페이지의 기사 제목 수집하기\n",
    "1,2,3… 등으로 구분된 페이지에 있는 기사들의 제목을 한번에 수집하고자 합니다.\n",
    "\n",
    "실습을 진행하는 웹 사이트 주소입니다. https://sports.donga.com/ent?p=1&c=02"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 지시사항\n",
    "main 함수에서 req 변수를 올바르게 초기화하세요.\n",
    "\n",
    "crawling 함수를 올바르게 작성하세요.\n",
    "\n",
    "Tip!\n",
    "requests.get메소드의 params 매개변수로 여러 페이지에 접근하시거나, url을 문자열 연산으로 조작하여 여러 페이지에 접근할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['고현정, 딸 김수안 말에 불안 눈빛 (‘너닮사’)', '데프콘 “제발 저 사람 사랑하게 해주세요” 마성남 누구? (나는 솔로)', '첫방 ‘미친.사랑.X’ 오은영 “치정 예능, 출연 고민해”', '허니제이 “리헤이와 원수? 싸운 적 없어” (유퀴즈)[TV체크]', '김민재, ♥박규영 위한 특급 요리 (‘달리와 감자탕’)', '박규영, 김민재·권율 신경전에 고심 (‘달리와 감자탕’)', '허니제이 “우승 상금으로 수술한 멤버 지원” [TV체크]', '‘첫 패배’ FC 아나콘다, 독기 품고 폭주 (‘골때녀2’)', '김지석, 임성빈과 환장 케미 제주行 (‘구해줘!숙소’)', '송소희X황소윤, 최종병기 등판 (‘골때녀’)', '‘스우파’ 리더 7인, 유재석X조세호와 합동 무대 (유퀴즈)', '이영현 “첫 자작곡 ‘체념’, 며칠을 울며 써”', '임수정X이도현 “나랑 하자, 수학” 하이라이트 공개 (멜랑꼴리아)', '이승윤, ‘개승자’ 출격…자연인 아닌 본업 복귀', '김요한X추영우, 김우빈·이종석 브로맨스 잇는다 (학교2021)', '‘디피’ 현봉식, 귀여운 본명 공개 “현보람” (라스)', '라붐 소연, 랜덤 플레이 댄스 중 호통 (‘주간아’)', '저스트비, ‘틱톡’ 첫 주 활동 성료', '‘라우드’, Korea UHD Award 엔터테인먼트 최우수상', '배해선VS이학주, 날카로운 견제 눈빛 (‘청와대로 간다’)', '‘구경이’ 이영애VS김혜준, 애니 오프닝의 비밀은?', '빅스 레오, 아이튠즈 7개국 K팝 차트 진입', '트레저, 서늘한 개인 포스터 (‘남고괴담’)', '블랙핑크, 기후 행동 촉구…YG도 변화 시작', '드리핀, 야성美 장착', '유인나 MC확정 (‘시나브로 꿈 조작단’)', '슈퍼주니어-D&E, 아이튠즈 톱 앨범 22개 지역 1위', '버나드박, 세련된 영상美 티저 공개', '길구봉구, 소속사 이적 후 8일 첫 컴백', '힐링 다큐 ‘한창나이 선녀님’, 연이은 단체 관람 행렬', '박해수, 무한 연기 변신 (‘키마이라’)', '이승윤, 앨범 작업기 공개…첫 셀프캠', '라붐 해인 ‘예쁨 장착’ [포토]', '해인 ‘꽃 처럼 보이나요?’ [포토]', '라붐 소연-해인 ‘러블리함 더하기’ [포토]', '소연 ‘도발적인 손하트’ [포토]', '라붐 소연 ‘활짝 핀 꽃송이’ [포토]', '라붐 솔빈 ‘사랑스러운 막내’ [포토]', '라붐 소연 ‘오랜만에 컴백 쇼케이스’ [포토]', '라붐 솔빈 ‘떨려요’ [포토]', '라붐 진예 ‘긴장되는 포토타임’ [포토]', '라붐 솔빈 ‘막내의 발랄함’ [포토]', '라붐 진예 ‘새 소속사로 돌아왔어요’ [포토]', '라붐 해인 ‘귀여운 윙크’ [포토]', '라붐 ‘여신의 귀환’ [포토]', '라붐 ‘4인조로 돌아왔어요’ [포토]', '라붐 ‘러블리함 가득’ [포토]', '라붐 ‘애교가 팡팡’ [포토]', '라붐의 시그니처 포즈 [포토]', '라붐 ‘새 소속사에서 4인조로 컴백’ [포토]', '전소미, 스포티파이 글로벌 앨범 3위', '라붐 ‘꽃향기 퍼지는 ’KISS KISS‘’ [포토]', '라붐 ‘키스키스’ [포토]', '킹덤, 美 빌보드 ‘월드 디지털 송 세일즈’ 7위', '라붐 ‘발랄함 가득’ [포토]', '라붐 ‘러블리함 폭발’ [포토]', '세븐틴, 美 ‘빌보드200’ 13위+3개 차트 1위', '엔하이픈, 美 ‘빌보드200’ 2주 연속 차트인', '비투비 이민혁, ‘알아’ 무빙 포스터 공개', '‘지헤중’ 감독 “제목 의미? 인생은 헤어짐+만남 연속”', '‘5주년’ 빅톤, 12월 팬미팅 개최 [공식]', '최영준 “‘빈센조’ 송중기와 액션, 발 썼다” (라스)', '박시연, 두 번째 음주사고 후 근황…환한 미소 [DA★]', '정일우 홍보대사, 프랑스 드라마 페스티벌 개최', '김민주, 히메컷도 소화한 미모 [화보]', '‘박유천 동생’ 박유환, 대마초 혐의로 입건 [종합]', '“로아 세상에 온 이유는…” 조윤희 감동+뭉클 (내가 키운다)', '박솔미, NEW 편세프 합류 (편스토랑) [공식]', '박강섭, 푸근한 하트 (어사와 조이) [DA포토]', '민진웅, 웃음 가득한 인사 (어사와 조이) [DA포토]', '이상희, 화사한 미소 (어사와 조이) [DA포토]', '옥택연→김혜윤, 화기애애한 ‘어사와 조이’ [DA포토]', '옥택연→김혜윤, ‘어사와 조이’ 많이 기대해주세요 [DA포토]', '옥택연·김혜윤, 러블리 하트♥ (어사와 조이) [DA포토]', '옥택연·김혜윤, 기대되는 케미 (어사와 조이) [DA포토]', '김혜윤, 사랑스러운 미소 (어사와 조이) [DA포토]', '김혜윤, 해맑은 손하트♥ (어사와 조이) [DA포토]', '옥택연, 잘생긴 암행어사 (어사와 조이) [DA포토]', '옥택연, 훈훈한 손인사 (어사와 조이) [DA포토]', '방탄소년단 4억뷰,  ‘Life Goes On’ 뮤비', '방탄소년단 진 OST 7일 발매 [공식]', '트와이스 개인 티저 이미지 추가 오픈', '유승목 전속계약, 강호동 한솥밥 [공식]', '출연진 프로필 공개, 연령+스펙 美쳤다 (러브캐처 인 서울)', '백지영, 전국투어 콘서트 개최 [공식]', '워너원 재결합하나 “MAMA·음반·콘서트 논의” [공식입장]', '이성국, ‘도전 꿈의 무대’ 왕중왕전 출격', '정준하(MC 민지), 새 싱글 두 번째 티저 공개', '[단독] 김하늘, 새 드라마 ‘킬힐’ 주연', '전소미·딘딘 ‘골목식당’ 출격', '김지운·이선균 애플티비+ ‘닥터브레인’ 첫선…넷플·티빙 게 섰거라! [종합]', '이선균→이유영 “애플티비+ 첫 韓 작품 영광” (닥터브레인)', '김지운 감독 “이선균=국민배우, 박희순은 연륜+섹시” (닥터브레인)', \"이선균→서지혜, 미스터리 SF 스릴러 '닥터브레인' [DA포토]\", '이재원, 훈훈한 비주얼 (닥터브레인) [DA포토]', '이유영, 우아한 미모 (닥터브레인) [DA포토]', '김지운 감독 “원작 웹툰에 흥미, 첫 드라마 데뷔 흥분” (닥터브레인)']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def crawling(soup) :\n",
    "    result = []\n",
    "    ul = soup.find(\"ul\", class_= \"list_news\")\n",
    "    \n",
    "    for span in ul.find_all(\"span\", class_=\"tit\"):\n",
    "        result.append(span.get_text())\n",
    "        \n",
    "    # soup 객체에서 추출해야 하는 정보를 찾고 반환하세요.\n",
    "    return result\n",
    "    \n",
    "\n",
    "\n",
    "def main() :\n",
    "    answer = []\n",
    "    url = \"https://sports.donga.com/ent\"\n",
    "    \n",
    "    for i in range(0, 5):\n",
    "        req = requests.get(url, params={'p': i*20+1})\n",
    "        soup = BeautifulSoup(req.text, \"html.parser\")\n",
    "        \n",
    "        answer += crawling(soup)\n",
    "\n",
    "    # crawling 함수의 결과를 출력합니다.\n",
    "    print(answer)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\" :\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5차시 크롤링 실습1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 각 기사의 href 수집하기\n",
    "이전 실습에서 진행된 1,2,3 — 으로 구성된 웹 사이트 형태뿐만이 아니라\n",
    "\n",
    "단일 페이지에 여러 가지 링크가 있는 경우가 있습니다. 기사를 클릭하면 해당 기사를 볼 수 있는 url로 이동할 수 있습니다. 이런 url로 이동하는 링크들을 수집하고자 합니다.\n",
    "\n",
    "HTML Tag 중, 연동된 href를 수집하여 리스트형 변수 list_href에 담아 출력하는 실습을 진행합니다.\n",
    "\n",
    "https://sports.donga.com/ent?p=1&c=02\n",
    "\n",
    "출력 예시\n",
    "[\"http://——\", \"http://———\", ——]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 지시사항\n",
    "get_href는 soup 객체를 받아서, 뉴스 기사들로 접근할 수 있는 href 링크를 리스트에 담아 반환해야 합니다.\n",
    "\n",
    "- Tips!\n",
    "어떤 태그가 갖고 있는 속성들은 딕셔너리의 형태로 저장됩니다.\n",
    "\n",
    "예를 들어, a 태그가 href, onclick의 속성을 갖고 있다면 그 속성들은\n",
    "\n",
    "{\"href\" : ... , \"onclick\" : ...}\n",
    "\n",
    "과 같이 표현됩니다.\n",
    "\n",
    "- 어떤 태그 a의 속성 href에 바로 접근하는 방법은\n",
    "\n",
    "a = soup.find(\"a\")\n",
    "a[\"href\"]\n",
    "Copy\n",
    "위와 같이 대괄호와 속성 이름을 명시하는 것입니다.\n",
    "\n",
    "- 전체 속성을 담고 있는 딕셔너리를 확인하기 위해서는\n",
    "\n",
    "a.attrs\n",
    "Copy\n",
    "로 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://sports.donga.com/ent/article/all/20211103/110066325/1', 'https://sports.donga.com/ent/article/all/20211103/110066090/1', 'https://sports.donga.com/ent/article/all/20211103/110065752/1', 'https://sports.donga.com/ent/article/all/20211103/110067508/1', 'https://sports.donga.com/ent/article/all/20211103/110065857/1', 'https://sports.donga.com/ent/article/all/20211103/110064229/1', 'https://sports.donga.com/ent/article/all/20211103/110067132/1', 'https://sports.donga.com/ent/article/all/20211103/110066122/1', 'https://sports.donga.com/ent/article/all/20211103/110066354/1', 'https://sports.donga.com/ent/article/all/20211103/110065715/1', 'https://sports.donga.com/ent/article/all/20211103/110066343/1', 'https://sports.donga.com/ent/article/all/20211103/110064378/1', 'https://sports.donga.com/ent/article/all/20211103/110066310/1', 'https://sports.donga.com/ent/article/all/20211103/110066290/1', 'https://sports.donga.com/ent/article/all/20211103/110066244/1', 'https://sports.donga.com/ent/article/all/20211103/110066237/1', 'https://sports.donga.com/ent/article/all/20211103/110065956/1', 'https://sports.donga.com/ent/article/all/20211103/110066217/1', 'https://sports.donga.com/ent/article/all/20211103/110066164/1', 'https://sports.donga.com/ent/article/all/20211103/110066155/1']\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_href(soup) :\n",
    "    # soup에 저장되어 있는 각 기사에 접근할 수 있는 href들을 담고 있는 리스트를 반환해주세요.\n",
    "    \n",
    "    ul = soup.find(\"ul\", class_=\"list_news\")\n",
    "    result = []\n",
    "    \n",
    "    for span in ul.find_all('span', class_=\"tit\"):\n",
    "        result.append(span.find(\"a\")[\"href\"])\n",
    "    return result\n",
    "\n",
    "def main():\n",
    "    list_href = []\n",
    "\n",
    "    url = \"https://sports.donga.com/ent?p=1&c=02\"\n",
    "    result = requests.get(url)\n",
    "    soup = BeautifulSoup(result.text, \"html.parser\")\n",
    "\n",
    "    list_href = get_href(soup)\n",
    "    print(list_href)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6차시 크롤링실습2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 네이트 최신뉴스 href 수집하기\n",
    "웹 페이지 href 링크들을 수집하여 리스트형 변수 list_href에 담아 출력해봅니다.\n",
    "\n",
    "https://news.nate.com/recent?mid=n0100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 지시사항\n",
    "- a 태그가 있는 div 태그 및 class를 찾아보세요.\n",
    "\n",
    "- find(\"a\")[\"href\"]로 속성값 href의 데이터를 추출해보세요.\n",
    "\n",
    "- 추출한 href 문자열 앞에 \"https:\" 를 앞에 붙여 완벽한 링크로 만들어보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://news.nate.com/view/20211103n40470?mid=n0100', 'https://news.nate.com/view/20211103n26630?mid=n0100', 'https://news.nate.com/view/20211103n40468?mid=n0100', 'https://news.nate.com/view/20211103n40467?mid=n0100', 'https://news.nate.com/view/20211103n40466?mid=n0100', 'https://news.nate.com/view/20211103n40465?mid=n0100', 'https://news.nate.com/view/20211103n40464?mid=n0100', 'https://news.nate.com/view/20211103n40435?mid=n0100', 'https://news.nate.com/view/20211103n40171?mid=n0100', 'https://news.nate.com/view/20211103n40461?mid=n0100', 'https://news.nate.com/view/20211103n40105?mid=n0100', 'https://news.nate.com/view/20211103n40457?mid=n0100', 'https://news.nate.com/view/20211103n26586?mid=n0100', 'https://news.nate.com/view/20211103n40455?mid=n0100', 'https://news.nate.com/view/20211103n40398?mid=n0100', 'https://news.nate.com/view/20211103n40453?mid=n0100', 'https://news.nate.com/view/20211103n40450?mid=n0100', 'https://news.nate.com/view/20211103n33426?mid=n0100', 'https://news.nate.com/view/20211103n40447?mid=n0100', 'https://news.nate.com/view/20211103n40446?mid=n0100']\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "    \n",
    "def get_href(soup) :\n",
    "    # 각 기사에 접근할 수 있는 href를 리스트로 반환하세요.\n",
    "    result = []\n",
    "    div_list = soup.find_all(\"div\", class_ = \"mduSubjectList\")\n",
    "    \n",
    "    for div in div_list:\n",
    "        result.append(\"https:\"+div.find(\"a\")[\"href\"])\n",
    "    return result\n",
    "    \n",
    "\n",
    "def main() :\n",
    "    list_href = []\n",
    "    \n",
    "    # href 수집할 사이트 주소 입력\n",
    "    url = \"https://news.nate.com/recent?mid=n0100\"\n",
    "    req = requests.get(url)\n",
    "    soup = BeautifulSoup(req.text, \"html.parser\")\n",
    "    \n",
    "    list_href = get_href(soup)\n",
    "    \n",
    "    print(list_href)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\" :\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7차시 크롤링실습3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 다양한 섹션의 속보 기사 href 추출하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "“정치”, “경제”, “사회”, “생활”, “세계”, “과학” 으로 나뉘어진 다양한 분야의 속보 기사를 추출하고자 합니다.\n",
    "\n",
    "https://news.naver.com/main/list.nhn?sid1=100\n",
    "\n",
    "위 url에서, sid1 부분으로 분야를 설정할 수 있습니다.\n",
    "\n",
    "sid1\t분야\tsid1\t분야\n",
    "100\t정치\t103\t생활\n",
    "101\t경제\t104\t세계\n",
    "102\t사회\t105\t과학"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 지시사항\n",
    "* get_request 함수를 올바르게 구현하세요. get_request 함수에 문자열 형태의 분야 이름이 들어가면, 해당 분야의 뉴스 속보 홈페이지의 GET 요청의 응답을 반환해야 합니다.\n",
    "\n",
    "* get_href 함수를 올바르게 구현하세요. get_request 함수로부터 특정 분야에 대한 GET 요청의 응답을 올바르게 받았다면, get_href는 분야별 속보 기사에 접근할 수 있는 href를 리스트로 반환해야 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tip!\n",
    "- 이번 실습의 코드를 살펴보면 이전까지의 실습에선 볼 수 없었던 새로운 함수 get_request를 발견하실 수 있습니다.\n",
    "\n",
    "- main 함수의 req 변수는 get_request 함수의 반환값을 갖습니다.\n",
    "\n",
    "- get_request는 requests.get() 함수를 실행한 결과(응답)을 반환합니다.\n",
    "\n",
    "- 기본 url에, 입력된 분야에 맞게끔 params 매개변수를 설정하시거나 문자열 연산을 통해 분야별 속보 페이지에 접근할 수 있도록 하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_href(soup) :\n",
    "    # 각 분야별 속보 기사에 접근할 수 있는 href를 리스트로 반환하세요.\n",
    "    ul = soup.find(\"ul\", class_=\"type06_headline\")\n",
    "    result = []\n",
    "    \n",
    "    for a in ul.find_all(\"a\", class_=\"nclicks(fls.list)\"):\n",
    "        result.append(a[\"href\"])\n",
    "    return result\n",
    "    \n",
    "def get_request(section) :\n",
    "    # 입력된 분야에 맞는 request 객체를 반환하세요.\n",
    "    # 아래 url에 쿼리를 적용한 것을 반환합니다.\n",
    "    custom_header = {\n",
    "        'referer' : 'https://www.naver.com/',\n",
    "        'user-agent' : 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.103 Safari/537.36'\n",
    "    }\n",
    "    \n",
    "    url = \"https://news.naver.com/main/list.nhn\"\n",
    "    sections = {\n",
    "        \"정치\" : 100,\n",
    "        \"경제\" : 101,\n",
    "        \"사회\" : 102,\n",
    "        \"생활\" : 103,\n",
    "        \"세계\" : 104,\n",
    "        \"과학\" : 105\n",
    "    \n",
    "    }\n",
    "    \n",
    "    req = requests.get(url, headers = custom_header,\n",
    "            params = {\"sid1\": sections[section]}) # params 매개변수를 올바르게 설정하세요.\n",
    "    return req\n",
    "\n",
    "def main() :\n",
    "    list_href = []\n",
    "    \n",
    "    # 섹션을 입력하세요.\n",
    "    section = input('\"정치\", \"경제\", \"사회\", \"생활\", \"세계\", \"과학\" 중 하나를 입력하세요.\\n  > ')\n",
    "    \n",
    "    req = get_request(section)\n",
    "    soup = BeautifulSoup(req.text, \"html.parser\")\n",
    "    \n",
    "    list_href = get_href(soup)\n",
    "    \n",
    "    print(list_href)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\" :\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8차시 크롤링실습4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 다양한 섹션의 속보 기사 내용 추출하기\n",
    "다양한 섹션의 속보 기사 href 추출하기 실습과 마찬가지로 네이버 뉴스 속보 페이지에서 실습을 진행합니다.\n",
    "\n",
    "사용 url :\n",
    "https://news.naver.com/main/list.nhn\n",
    "\n",
    "이번에는 특정 분야를 입력받으면 해당 분야의 속보 기사들의 href를 얻고, 그 href로 각각의 기사로 접근하여 기사의 내용을 크롤링하려고 합니다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 지시사항\n",
    "- get_request, get_href, crawling 함수를 올바르게 구현하세요.\n",
    "\n",
    "- get_request : 문자열 형태의 분야 이름이 매개변수로 주어지면, 해당 분야의 뉴스 속보 홈페이지의 GET 요청의 응답을 반환해야 합니다.\n",
    "\n",
    "- get_href : get_request 함수로부터 특정 분야에 대한 GET 요청의 응답을 올바르게 받았다면, get_href는 분야별 속보 기사에 접근할 수 있는 href를 리스트로 반환해야 합니다.\n",
    "\n",
    "- crawling : get_href 함수로부터 얻은 각 기사의 href로 기사에 접근하여 기사의 내용을 리스트로 반환해야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def crawling(soup) :\n",
    "    # 기사에서 내용을 추출하고 반환하세요.\n",
    "    div = soup.find(\"div\", class_=\"_article_body_contents\")\n",
    "    \n",
    "    result = div.get_text().replace('\\n','').replace('// flash 오류를 우회하기 위한 함수 추가function_flash_removeCallback() {}','').replace('\\t','')\n",
    "    return result\n",
    "\n",
    "def get_href(soup) :\n",
    "    # 각 분야별 속보 기사에 접근할 수 있는 href를 리스트로 반환하세요.\n",
    "    result = []\n",
    "    \n",
    "    for ul in soup.find_all(\"ul\", class_=\"type06_headline\"):\n",
    "        for a in ul.find_all(\"a\"):\n",
    "            result.append(a[\"href\"])\n",
    "    return result\n",
    "\n",
    "def get_request(section) :\n",
    "    # 입력된 분야에 맞는 request 객체를 반환하세요.\n",
    "    # 아래 url에 쿼리를 적용한 것을 반환합니다.\n",
    "    custom_header = {\n",
    "        'referer' : 'https://www.naver.com/',\n",
    "        'user-agent' : 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.103 Safari/537.36'\n",
    "    }\n",
    "    \n",
    "    sections = { \"정치\" : 100,\n",
    "                \"경제\" : 101,\n",
    "                \"사회\" : 102,\n",
    "                \"생활\" : 103,\n",
    "                \"세계\" : 104,\n",
    "                \"과학\" : 105}\n",
    "    url = \"https://news.naver.com/main/list.nhn\"\n",
    "    \n",
    "    req = requests.get(url, headers = custom_header,\n",
    "            params = {\"sid1\" : sections[section]}) # params 매개변수를 올바르게 설정하세요.\n",
    "    return req\n",
    "    \n",
    "\n",
    "def main() :\n",
    "    custom_header = {\n",
    "        'referer' : 'https://www.naver.com/',\n",
    "        'user-agent' : 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.103 Safari/537.36'\n",
    "    }\n",
    "    \n",
    "    list_href = []\n",
    "    result = []\n",
    "    \n",
    "    # 섹션을 입력하세요.\n",
    "    section = input('\"정치\", \"경제\", \"사회\", \"생활\", \"세계\", \"과학\" 중 하나를 입력하세요.\\n  > ')\n",
    "    \n",
    "    req = get_request(section)\n",
    "    soup = BeautifulSoup(req.text, \"html.parser\")\n",
    "    \n",
    "    list_href = get_href(soup)\n",
    "    \n",
    "    for href in list_href :\n",
    "        href_req = requests.get(href, headers = custom_header)\n",
    "        href_soup = BeautifulSoup(href_req.text, \"html.parser\")\n",
    "        result.append(crawling(href_soup))\n",
    "    print(result)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\" :\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9차시 크롤링 실습 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 특정 영화 리뷰 추출하기\n",
    "리뷰를 알고 싶은 영화의 제목을 입력하면, 해당 영화의 리뷰들의 제목을 알려주는 프로그램을 제작해봅시다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 지시사항\n",
    "- get_url, get_href, crawling 함수를 올바르게 구현하세요.\n",
    "\n",
    "- get_url : main 함수에서 입력된 영화 제목을 네이버 영화 검색창에 검색하였을 대 나오는 url을 반환해야 합니다.\n",
    "\n",
    "- get_href : get_url에서 얻은 url로 접근하였을 때, 가장 위에 존재하는 영화의 href를 반환합니다.\n",
    "\n",
    "- crawling : 이전에 구현하였던 영화 리뷰 추출 방식과 동일합니다.\n",
    "#### Tip!\n",
    "- get_href 함수를 구현하실 때 유의하실 점이 있습니다.\n",
    "\n",
    "- 이것은 어떤 영화의 기본 정보를 보여주는 URL입니다.\n",
    "\n",
    "- https://movie.naver.com/movie/bi/mi/basic.naver?code=영화코드 의 꼴입니다.\n",
    "\n",
    "- 예시) https://movie.naver.com/movie/bi/mi/basic.naver?code=168058\n",
    "\n",
    "- 이 URL은 어떤 영화의 리뷰 목록을 보여주는 URL입니다.\n",
    "\n",
    "- https://movie.naver.com/movie/bi/mi/review.naver?code=영화코드\n",
    "\n",
    "- 즉, 영화의 href를 얻고 나서, basic 부분을 review로 올바르게 바꿔주어야 리뷰에 접근할 수 있다는 의미입니다.\n",
    "\n",
    "- 예시) https://movie.naver.com/movie/bi/mi/review.naver?code=168058\n",
    "\n",
    "- 영화 사이트 url 변경으로 인해, url의 “nhn”이 “naver”로 변경되었습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def crawling(soup) :\n",
    "    # soup 객체에서 추출해야 하는 정보를 찾고 반환하세요.\n",
    "    result = []\n",
    "    ul = soup.find(\"ul\", class_=\"rvw_list_area\")\n",
    "    \n",
    "    for li in ul.find_all(\"li\"):\n",
    "        result.append(li.find(\"strong\").get_text())\n",
    "    return result\n",
    "    \n",
    "def get_href(soup) :\n",
    "    # 검색 결과, 가장 위에 있는 영화로 접근할 수 있는 href를 반환하세요.\n",
    "    ul = soup.find(\"ul\", class_=\"search_list_1\")\n",
    "    a = ul.find(\"a\")\n",
    "    href = a['href'].replace('basic', 'review')\n",
    "    return \"https://movie.naver.com\"+ href\n",
    "    \n",
    "\n",
    "def get_url(movie) :\n",
    "    # 입력된 영화를 검색한 결과의 url을 반환하세요.\n",
    "    return f\"https://movie.naver.com/movie/search/result.naver?query={movie}&section=all&ie=utf8\"\n",
    "    \n",
    "def main() :\n",
    "    list_href = []\n",
    "    \n",
    "    custom_header = {\n",
    "        'referer' : 'https://www.naver.com/',\n",
    "        'user-agent' : 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.103 Safari/537.36'\n",
    "    }\n",
    "    \n",
    "    # 섹션을 입력하세요.\n",
    "    movie = input('영화 제목을 입력하세요. \\n  > ')\n",
    "    \n",
    "    url = get_url(movie)\n",
    "    print(url)\n",
    "    req = requests.get(url, headers = custom_header)\n",
    "    soup = BeautifulSoup(req.text, \"html.parser\")\n",
    "    \n",
    "    movie_url = get_href(soup)\n",
    "    \n",
    "    href_req = requests.get(movie_url)\n",
    "    href_soup = BeautifulSoup(href_req.text, \"html.parser\")\n",
    "    \n",
    "    list_href = crawling(href_soup)\n",
    "    print(list_href)\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\" :\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10차시 API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Daum 증권 페이지에서 주가 크롤링\n",
    "- http://finance.daum.net/ 에서 보여주는 인기 검색 상위 10개 기업의 결과는, 이전처럼 HTML 문서를 분석하여 크롤링하는 방식을 사용할 수 없습니다.\n",
    "\n",
    "웹 페이지가 API로부터 실시간으로 변하는 주식의 정보를 주기적으로 요청하여 표시하고 있기 때문입니다.\n",
    "\n",
    "웹에서 요청하는 서버에 직접 요청하여 json 데이터를 얻은 후 출력해보도록 하겠습니다.\n",
    "\n",
    "API의 URL은 개발자 도구의 Network 탭에서 얻을 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 지시사항\n",
    "- get_data 함수를 올바르게 구현하세요.\n",
    "\n",
    "- get_data : json 파일로부터 원하는 데이터를 추출한 리스트를 반환합니다.\n",
    "    \n",
    "#### Tip!\n",
    "- json 파일을 파이썬 코드에서 불러오기 위해 파이썬의 json 모듈을 사용할 수 있습니다.\n",
    "\n",
    "import json\n",
    "with open(\"stock.json\") as stock :\n",
    "    data = json.loads(stock) \n",
    "Copy\n",
    "stock.json이라는 json 데이터를 data 변수에 로드하는 코드입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "custom_header = {\n",
    "    'referer' : 'https://cafe.naver.com/bppentest/book5109610/294',\n",
    "    'user-agent' : 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.103 Safari/537.36'  }\n",
    "\n",
    "\n",
    "def get_data() :\n",
    "    result https://cafe.naver.com/bppentest/book5109610/294\" # 상위 10개 기업의 정보를 얻는 API url을 작성하세요.\n",
    "    req = requests.get(url, headers = custom_header)\n",
    "    \n",
    "    if req.status_code == requests.codes.ok:    \n",
    "        print(\"접속 성공\")\n",
    "        # API에 접속에 성공하였을 때의 logic을 작성해주세요.\n",
    "        # JSON 데이터의 원하는 부분만 불러와 result에 저장해주세요.\n",
    "        \n",
    "        stock_data = json.loads(req.text)\n",
    "        for d in stock_data[\"data\"]:\n",
    "            result.append([d['rank'], d['name'], d['tradePrice']])\n",
    "        \n",
    "        \n",
    "    else:\n",
    "        print(\"접속 실패\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "def main() :\n",
    "    data = get_data()\n",
    "    \n",
    "    for d in data :\n",
    "        print(d)\n",
    "    \n",
    "if __name__ == \"__main__\" :\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11주차 프로젝트 (음식점 리뷰 크롤링)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 주의사항\n",
    "- 실습 url : https://www.mangoplate.com/restaurants/B8CzA6i9Bb8Z\n",
    "- 실습에서 사용한 url을 통해 음식점의 리뷰를 크롤링하고 싶으시다면,\n",
    "- main 함수의 href 변수에 /restaurants/B8CzA6i9Bb8Z를 넣어주시면 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 지시사항\n",
    "- get_reviews 함수를 올바르게 작성하세요.\n",
    "\n",
    "- get_reviews : 음식점 리뷰 데이터를 리스트에 담아 반환합니다.\n",
    "    \n",
    "#### Tip!\n",
    "- get_reviews 함수의 지역변수 url은 망고플레이트의 api url을 갖고 있습니다.\n",
    "\n",
    "- 해당 api에 get 요청을 보내면 특정 restaurant id의 리뷰를 반환합니다.\n",
    "\n",
    "- 이때 반환하는 리뷰의 시작 인덱스는 start_index, 리뷰의 개수는 request_count 변수의 값으로 조절합니다.\n",
    "\n",
    "- start_index 값은 main 함수 내의 i 변수로 조절하고 있고, request_count는 50으로 고정된 값을 설정했습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import json\n",
    "\n",
    "custom_header = {\n",
    "    'referer' : 'https://www.mangoplate.com',\n",
    "    'user-agent' : 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.103 Safari/537.36' }\n",
    "\n",
    "def get_reviews(code) :\n",
    "    comments = []\n",
    "    \n",
    "    i = 0\n",
    "    while True :\n",
    "        url = f\"https://stage.mangoplate.com/api/v5{code}/reviews.json?language=kor&device_uuid=V3QHS15862342340433605ldDed&device_type=web&start_index={i}&request_count=50&sort_by=2\"\n",
    "        req = requests.get(url, headers = custom_header)\n",
    "        \n",
    "        if req.status_code == requests.codes.ok:    \n",
    "            print(\"접속 성공\")\n",
    "            reviews = json.loads(req.text)\n",
    "            \n",
    "            if len(reviews) == 0:\n",
    "                break\n",
    "            \n",
    "            for review in reviews:\n",
    "                comment = review[\"comment\"]\n",
    "                text = comment[\"comment\"]\n",
    "                comments.append(comment)\n",
    "            \n",
    "        else:\n",
    "            print(\"Error code\")\n",
    "        i = i+5\n",
    "    return comments\n",
    "    \n",
    "    \n",
    "\n",
    "def main() :\n",
    "    href = \"/restaurants/B8CzA6i9Bb8Z\"\n",
    "    print(get_reviews(href))\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\" :\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 음식점 href 크롤링\n",
    "- 이전에는 크롤링하고 싶은 음식점의 고유 번호 값을 직접 입력해주어야 했기에 크롤링하기 불편했습니다.\n",
    "\n",
    "- 여러 음식점을 손쉽게 크롤링할 수 있도록 특정 키워드를 검색하였을 때 검색 결과로 나타나는 식당들의 이름과 href 링크 정보를 얻어와봅시다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 지시사항\n",
    "- get_restaurants함수를 올바르게 작성하세요\n",
    "\n",
    "- get_restaurants : 매개변수로 검색어가 주어질 때, 검색 결과로 나타나는 식당의 이름과 href들을 리스트에 담아 반환합니다.\n",
    "\n",
    "    #### 반환값 예시\n",
    "[(식당 이름1, href1), (식당 이름2, href2), ...]\n",
    "Copy\n",
    "requests에 header를 지정해주어야 올바르게 동작합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import json            #json import하기\n",
    "\n",
    "custom_header = {\n",
    "    'referer' : 'https://www.mangoplate.com/',\n",
    "    'user-agent' : 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.103 Safari/537.36' }\n",
    "\n",
    "\n",
    "def get_restaurants(name) :\n",
    "    # 검색어 name이 들어왔을 때 검색 결과로 나타나는 식당들을 리스트에 담아 반환하세요.\n",
    "    restaurant_list = []\n",
    "    url = \"https://www.mangoplate.com/search/\"+name\n",
    "    req = requests.get(url,headers = custom_header)\n",
    "    soup = BeautifulSoup(req.text, \"html.parser\")\n",
    "    \n",
    "    restaurant = soup.find_all(\"div\", class_=\"list-restaurant-item\")\n",
    "    \n",
    "    for rest in restaurant :\n",
    "        info = rest.find(\"div\",class_=\"info\")\n",
    "        href = info.find(\"a\")[\"href\"]\n",
    "        title = info.find(\"h2\").get_text().replace('\\n','').replace(' ','')\n",
    "        \n",
    "        restaurant_list.append([title,href])\n",
    "    return restaurant_list\n",
    "    \n",
    "\n",
    "def main() :\n",
    "    name = input()\n",
    "    \n",
    "    restaurant_list = get_restaurants(name)\n",
    "    \n",
    "    print(restaurant_list)\n",
    "\n",
    "if __name__ == \"__main__\" :\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12주차 워드 클라우드"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 워드클라우드 출력하기\n",
    "- 아래 지시사항에 적힌 예시 코드를 참고하여 워드클라우드를 출력하는 함수 create_word_cloud를 구현해보세요.\n",
    "\n",
    "- create_word_cloud 함수는 문자열 데이터를 입력받고, 해당 문자열의 워드클라우드를 출력합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 지시사항\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "워드클라우드를 그리기 위해서 WordCloud모듈이 필요합니다.\n",
    "\n",
    "WordCloud의 fit_words라는 함수가 단어들의 빈도 수가 담긴 딕셔너리를 매개변수로 받아, 워드클라우드를 그리는 역할을 합니다.\n",
    "\n",
    "#### 1. 단어 빈도수 얻기\n",
    "counter = count_word_freq(data)\n",
    "Copy\n",
    "문자열 data에 들어있는 단어들의 빈도수를 얻어 count에 저장합니다.\n",
    "\n",
    "#### 2. 워드클라우드 객체 생성하기\n",
    "cloud = WordCloud(background_color='white')\n",
    "Copy\n",
    "배경색이 흰색인 WordCloud 객체를 생성합니다.\n",
    "\n",
    "#### 3. 워드클라우드 그리기\n",
    "cloud.fit_words(counter)\n",
    "Copy\n",
    "단어들의 횟수를 기반으로 워드클라우드를 생성합니다.\n",
    "\n",
    "cloud.to_file('cloud.png')\n",
    "elice_utils.send_image('cloud.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "from count import count_word_freq\n",
    "from text import data\n",
    "from elice_utils import EliceUtils\n",
    "elice_utils = EliceUtils()\n",
    "\n",
    "def create_word_cloud(data) :\n",
    "    counter = count_word_freq(data)\n",
    "    #코드를 작성하세요.\n",
    "    count = count_word_freq(data)\n",
    "    \n",
    "    cloud = WorldCloud(background_color = 'white')\n",
    "    cloud.fit_words(counter)\n",
    "    \n",
    "    cloud.to_file('cloud.png')\n",
    "    elice_utils.send_image('cloud.png')\n",
    "    \n",
    "    \n",
    "    return None\n",
    "    \n",
    "    \n",
    "if __name__ == \"__main__\" :\n",
    "    create_word_cloud(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13주차 네이버뉴스기사 워드클라우드"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 네이버 뉴스 기사 워드클라우드 출력하기\n",
    "앞에서 추출한 네이버 뉴스 기사의 내용에서 워드클라우드를 출력해보도록 하겠습니다.\n",
    "\n",
    "main함수의 url 변수에 분석하길 원하는 네이버 뉴스 기사 주소를 넣어 분석할 수 있습니다.\n",
    "\n",
    "예시\n",
    "image\n",
    "\n",
    "예시로 사용한 URL은 다음과 같습니다. : https://news.naver.com/main/read.nhn?mode=LSD&mid=shm&sid1=104&oid=421&aid=0004611563"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from wc import create_word_cloud\n",
    "\n",
    "def crawling(soup) :\n",
    "    # 기사에서 내용을 추출하고 반환하세요.\n",
    "    div = soup.find('div', class_=\"_article_body_contents\")\n",
    "    \n",
    "    result = div.get_text().replace('\\n', '').replace('// flash 오류를 우회하기 위한 함수 추가function _flash_removeCallback() {}', '').replace('\\t', '')\n",
    "    \n",
    "    return result\n",
    "    \n",
    "    \n",
    "def main() :\n",
    "    custom_header = {'user-agent' : 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.103 Safari/537.36'}\n",
    "    url = \"https://news.naver.com/main/read.nhn?mode=LSD&mid=shm&sid1=104&oid=421&aid=0004611563\"\n",
    "    req = requests.get(url, headers=custom_header)\n",
    "    soup = BeautifulSoup(req.text, \"html.parser\")\n",
    "    \n",
    "    text = crawling(soup)\n",
    "    create_word_cloud(text)\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\" :\n",
    "    main()\n",
    "`ㅡ`ㅡ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14주차 형태소 추출"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 형태소 추출하기\n",
    "image\n",
    "\n",
    "이전 실습에서 출력했던 한글 워드클라우드입니다.\n",
    "\n",
    "여기서 문제점은, 한국어의 단어엔 어미와 조사가 붙어, 각 형태소(morpheme)가 띄어쓰기 단위로 나눠지지 않는다는 문제가 있습니다.\n",
    "\n",
    "예를 들어, 대통령이 과 대통령은은 둘 다 대통령이라는 공통된 형태소로 분류되어야 합니다.\n",
    "\n",
    "또한, 잘, 무척 등 의미를 도출하기 어려운 부사들이 있어 텍스트 내에서 의미를 도출하기 쉬운 명사만 추출하고자 합니다.\n",
    "\n",
    "워드 클라우드는 키워드를 찾는 것이 중요하므로, 조사 등이 붙지 않은 형태소들 중 명사들을 결과로 워드클라우드를 그리려고 합니다.\n",
    "\n",
    "형태소만 추출하기 위해 파이썬 자연어 처리 라이브러리 중 하나인 mecab의 사용 방법을 알아보고자 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 지시사항\n",
    "mecab = MeCab() 으로 mecab 객체를 생성할 수 있습니다.\n",
    "\n",
    "mecab.morphs(text) 함수는, 매개변수로 들어온 문장을 형태소별로 나누어 리스트로 반환합니다.\n",
    "\n",
    "mecab.nouns(text) 함수는, 매개변수로 들어온 문장을 형태소별로 나누었을 때 명사만 추출하여 리스트로 반환합니다.\n",
    "\n",
    "mecab.pos(text) 함수는, 매개변수로 들어온 문장을 형태소별로 나누고, 각 형태소별로 품사에 대한 정보까지 포함하여 반환합니다.\n",
    "\n",
    "주어진 text 변수, 또는 자유롭게 text 변수의 값을 설정하여 mecab 모듈의 함수를 사용해보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mecab import MeCab\n",
    "mecab = MeCab()\n",
    "\n",
    "text = \"광화문역 주변에 있는 맛집을 알고 싶어요. 정보를 얻을 수 있을까요?\"\n",
    "\n",
    "# 1. 형태소 별로 나눠 출력해보기\n",
    "print(mecab.morphs(text))\n",
    "\n",
    "# 2. 명사만 출력해보기\n",
    "print(mecab.nouns(text))\n",
    "\n",
    "# 3. 형태소 별로 나누고 품사 출력해보기\n",
    "print(mecab.pos(text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 형태소를 추출한 워드클라우드 출력하기\n",
    "이번 워드클라우드는 주어진 텍스트 데이터에서 명사만 추출한 것으로 그려보도록 하겠습니다.\n",
    "\n",
    "count.py 파일의 count_word_freq 함수에 명사를 추출하는 코드를 추가해주셔야 합니다.\n",
    "\n",
    "main.py파일의 main 함수에서 사용한 url :\n",
    "https://news.naver.com/main/read.nhn?mode=LSD&mid=shm&sid1=101&oid=031&aid=0000556282"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from string import punctuation\n",
    "import mecab\n",
    "mecab = mecab.MeCab()\n",
    "\n",
    "def count_word_freq(data) :\n",
    "    _data = data.lower()\n",
    "    \n",
    "    for p in punctuation :\n",
    "        _data = _data.replace(p, \"\")\n",
    "    \n",
    "    # 명사를 추출하세요.\n",
    "    _data = mecab.nouns(_data)\n",
    "    \n",
    "    counter = Counter(_data)\n",
    "    \n",
    "    return counter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15주차 여러개의 기사내용 크롤링"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 여러 개의 기사 내용 크롤링하기\n",
    "이번에는 여러 기사들의 텍스트 데이터를 모아와서 워드클라우드를 출력해보겠습니다.\n",
    "\n",
    "뉴스 게시판은 “정치”, “경제”, “사회”, “생활”, “세계”, “과학” 분야로 나눠져 있습니다.\n",
    "\n",
    "분야를 입력하면(정치, 경제 등) 해당 분야에 해당하는 여러 기사들의 내용을 크롤링하려고 합니다.\n",
    "\n",
    "아래 지시사항의 내용을 읽고 코드를 작성해보세요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 지시사항\n",
    "image\n",
    "\n",
    "네이버 뉴스 페이지는 관련된 주제에 따라 여러 개의 기사를 묶어서 보여주고 있습니다.\n",
    "\n",
    "이번에는 각 분야(예 : 정치, 경제 등) 별 페이지에서 가장 상단에 보이는 주제(예 : 트럼프, 알지만 말할 수 없어) 에 해당하는 기사들의 텍스트 데이터를 크롤링하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def crawling(soup) :\n",
    "    # 기사에서 내용을 추출하고 반환하세요.\n",
    "    div = soup.find('div', class_=\"_article_body_contents\")\n",
    "    \n",
    "    result = div.get_text().replace('\\n', '').replace('// flash 오류를 우회하기 위한 함수 추가function _flash_removeCallback() {}', '').replace('\\t', '')\n",
    "    \n",
    "    return result\n",
    "    \n",
    "\n",
    "def get_href(soup) :\n",
    "    result = []\n",
    "    \n",
    "    ul = soup.find(\"ul\", class_=\"cluster_list\")\n",
    "    for div in ul.find_all('div', class_=\"cluster_text\"):\n",
    "        result.append(div.find(\"a\")[\"href\"])\n",
    "    return result\n",
    "\n",
    "\n",
    "def get_request(section, custom_header) :\n",
    "    url = \"https://news.naver.com/main/main.nhn\"\n",
    "    section_dict = { \"정치\" : 100,\n",
    "                     \"경제\" : 101,\n",
    "                     \"사회\" : 102,\n",
    "                     \"생활\" : 103,\n",
    "                     \"세계\" : 104,\n",
    "                     \"과학\" : 105 }\n",
    "    return requests.get(url, params={\"sid1\":section_dict[section]}, headers=custom_header)\n",
    "\n",
    "\n",
    "def main() :\n",
    "    custom_header = {'user-agent' : 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.103 Safari/537.36'}\n",
    "    list_href = []\n",
    "    result = []\n",
    "    \n",
    "    # 섹션을 입력하세요.\n",
    "    section = input('\"정치\", \"경제\", \"사회\", \"생활\", \"세계\", \"과학\" 중 하나를 입력하세요.\\n  > ')\n",
    "    \n",
    "    req = get_request(section, custom_header)\n",
    "    soup = BeautifulSoup(req.text, \"html.parser\")\n",
    "    \n",
    "    list_href = get_href(soup)\n",
    "    \n",
    "    for href in list_href :\n",
    "        href_req = requests.get(href, headers=custom_header)\n",
    "        href_soup = BeautifulSoup(href_req.text, \"html.parser\")\n",
    "        result.append(crawling(href_soup))\n",
    "    print(result)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\" :\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16주차 더 많은 기사 내용 크롤링"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 더 많은 기사 내용 크롤링하기\n",
    "이전에는 각 주제마다 최대 3~4개의 기사들의 텍스트 데이터만 크롤링하였습니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 지시사항\n",
    "각 분야별 페이지에서 최상단에 위치한 주제의 기사들을 크롤링하되, 해당 주제에 대한 ‘더 많은 기사’들의 텍스트 데이터까지 크롤링해보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def crawling(soup) :\n",
    "    # 기사에서 내용을 추출하고 반환하세요.\n",
    "    div = soup.find('div', class_=\"_article_body_contents\")\n",
    "    \n",
    "    result = div.get_text().replace('\\n', '').replace('// flash 오류를 우회하기 위한 함수 추가function _flash_removeCallback() {}', '').replace('\\t', '')\n",
    "    \n",
    "    return result\n",
    "    \n",
    "\n",
    "def get_href(soup) :\n",
    "    result = []\n",
    "    h2 = soup.find(\"h2\", class_='cluster_head_topic')\n",
    "    href = h2.find('a')[\"href\"]\n",
    "\n",
    "    url = \"https://news.naver.com\"\n",
    "    req = requests.get(url)\n",
    "    new_soup = BeautifulSoup(req.text,'html.parser')\n",
    "    ul = new_soup.find(\"ul\", class_ = \"type06_headline\")\n",
    "    \n",
    "    for a in ul.find_all('a'):\n",
    "        result.append(a[\"href\"])\n",
    "    return result\n",
    "\n",
    "\n",
    "def get_request(section, custom_header) :\n",
    "    url = \"https://news.naver.com/main/main.nhn\"\n",
    "    section_dict = { \"정치\" : 100,\n",
    "                     \"경제\" : 101,\n",
    "                     \"사회\" : 102,\n",
    "                     \"생활\" : 103,\n",
    "                     \"세계\" : 104,\n",
    "                     \"과학\" : 105 }\n",
    "    return requests.get(url, params={\"sid1\":section_dict[section]}, headers=custom_header)\n",
    "\n",
    "\n",
    "def main() :\n",
    "    custom_header = {'user-agent' : 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.103 Safari/537.36'}\n",
    "    list_href = []\n",
    "    result = []\n",
    "    \n",
    "    # 섹션을 입력하세요.\n",
    "    section = input('\"정치\", \"경제\", \"사회\", \"생활\", \"세계\", \"과학\" 중 하나를 입력하세요.\\n  > ')\n",
    "    \n",
    "    req = get_request(section, custom_header)\n",
    "    soup = BeautifulSoup(req.text, \"html.parser\")\n",
    "    \n",
    "    list_href = get_href(soup)\n",
    "    \n",
    "    for href in list_href :\n",
    "        href_req = requests.get(href, headers=custom_header)\n",
    "        href_soup = BeautifulSoup(href_req.text, \"html.parser\")\n",
    "        result.append(crawling(href_soup))\n",
    "    print(result)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\" :\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 더 많은 기사로 워드클라우드 출력하기\n",
    "더 많은 기사의 내용으로 워드클라우드를 출력해봅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from wc import create_word_cloud\n",
    "\n",
    "def crawling(soup) :\n",
    "    result = \"\"\n",
    "    for children in soup.find(\"div\", class_=\"_article_body_contents\").children :\n",
    "        if children.name == None :\n",
    "            result += children\n",
    "    \n",
    "    start = result.find(\"// TV플레이어\")\n",
    "    result = result[start+len(\"// TV플레이어\")+1:]\n",
    "    \n",
    "    end = result.find(\"// 본문 내용\")\n",
    "    result = result[:end]\n",
    "    \n",
    "    return result.replace(\"\\n\", \"\").replace(\"\\t\", \"\")\n",
    "    \n",
    "\n",
    "def get_href(soup) :\n",
    "    result = []\n",
    "    \n",
    "    cluster_head = soup.find(\"h2\", class_=\"cluster_head_topic\")\n",
    "    href = cluster_head.find(\"a\")[\"href\"]\n",
    "    print(href)\n",
    "    url = \"https://news.naver.com\" + href\n",
    "    req = requests.get(url)\n",
    "    new_soup = BeautifulSoup(req.text, \"html.parser\")\n",
    "    \n",
    "    main_content = new_soup.find(\"div\", id=\"main_content\")\n",
    "    \n",
    "    for ul in main_content.find_all(\"ul\") :\n",
    "        for a in ul.find_all(\"a\") :\n",
    "            result.append(a[\"href\"])\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "def get_request(section) :\n",
    "    url = \"https://news.naver.com/main/main.nhn\"\n",
    "    section_dict = { \"정치\" : 100,\n",
    "                     \"경제\" : 101,\n",
    "                     \"사회\" : 102,\n",
    "                     \"생활\" : 103,\n",
    "                     \"세계\" : 104,\n",
    "                     \"과학\" : 105 }\n",
    "    return requests.get(url, params={\"sid1\":section_dict[section]})\n",
    "\n",
    "\n",
    "def main() :\n",
    "    list_href = []\n",
    "    result = []\n",
    "    \n",
    "    # 섹션을 입력하세요.\n",
    "    section = input('\"정치\", \"경제\", \"사회\", \"생활\", \"세계\", \"과학\" 중 하나를 입력하세요.\\n  > ')\n",
    "    \n",
    "    req = get_request(section)\n",
    "    soup = BeautifulSoup(req.text, \"html.parser\")\n",
    "    \n",
    "    list_href = get_href(soup)\n",
    "    \n",
    "    for href in list_href :\n",
    "        href_req = requests.get(href)\n",
    "        href_soup = BeautifulSoup(href_req.text, \"html.parser\")\n",
    "        result.append(crawling(href_soup))\n",
    "    \n",
    "    text = \" \".join(result)\n",
    "    create_word_cloud(text)\n",
    "    \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\" :\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
